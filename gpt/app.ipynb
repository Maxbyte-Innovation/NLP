{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import credentials\n",
    "import url\n",
    "from flask import Flask, render_template, request\n",
    "from langchain.document_loaders import DirectoryLoader,  SeleniumURLLoader, YoutubeLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OPENAI_API_KEY']=credentials.api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = Flask(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import mysql.connector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Establish a database connection<br>\n",
    "connection = mysql.connector.connect(<br>\n",
    "    host=\"127.0.0.1\",<br>\n",
    "    user=\"root\",<br>\n",
    "    password=\"sieomysql\",<br>\n",
    "    database=\"maxbyte\"<br>\n",
    ")<br>\n",
    "# Create a cursor<br>\n",
    "cursor = connection.cursor()<br>\n",
    "# Execute SQL query<br>\n",
    "cursor.execute(\"SELECT * FROM file\")<br>\n",
    "result = cursor.fetchall()<br>\n",
    "data=pd.DataFrame(result, columns=['S.no','SupervisorComments','Reason','Comments','SolutionGiven','CheckDescription','MachineName','CheckType','DepartmentName'])<br>\n",
    "# data.to_csv('datacsv.csv')<br>\n",
    "file='datacsv.csv'<br>\n",
    "# Close the cursor and connection<br>\n",
    "cursor.close()<br>\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plitting docs into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter1=CharacterTextSplitter(chunk_size=2500,chunk_overlap=600) #PDF\n",
    "text_splitter2=CharacterTextSplitter(chunk_size=3900,chunk_overlap=500) #URL\n",
    "text_splitter3=CharacterTextSplitter(chunk_size=1000,chunk_overlap=100) #VDO\n",
    "text_splitter4=CharacterTextSplitter(chunk_size=3000,chunk_overlap=300) #CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loading files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader1= DirectoryLoader(\"./files/\",glob='**/*.pdf')\n",
    "# loader2 = SeleniumURLLoader(urls=url.urls)\n",
    "# loader3 = YoutubeLoader.from_youtube_url(\n",
    "#     \"https://www.youtube.com/watch?v=M0tZO_WN-o8\", add_video_info=True\n",
    "# )\n",
    "# loader4= DirectoryLoader(\"./files/\",glob='**/*.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "oading documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs1=loader1.load()\n",
    "# docs2=loader2.load()\n",
    "# docs3=loader3.load()\n",
    "# docs4=loader4.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts=text_splitter1.split_documents(docs1)\n",
    "# texts+=text_splitter2.split_documents(docs2)\n",
    "# texts+=text_splitter3.split_documents(docs3)\n",
    "texts+=text_splitter4.split_documents(docs4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "account for deprecation of LLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "# # Get the current date\n",
    "current_date = datetime.datetime.now().date()\n",
    "# Define the date after which the \n",
    "# model should be set to \"gpt-3.5-turbo\"\n",
    "target_date = datetime.date(2024, 6, 12)\n",
    "# Set the model variable based on the current date\n",
    "if current_date > target_date:\n",
    "    llm_model = \"gpt-3.5-turbo\"\n",
    "else:\n",
    "    llm_model = \"gpt-3.5-turbo-0301\"\n",
    "# llm_model = \"stabilityai/stablelm-3b-4e1t\"  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "conversation_memory = ConversationBufferWindowMemory(buffer_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "# storing the texts\n",
    "vectorstore = FAISS.from_documents(texts, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "defining prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end. \n",
    "If the question is a greeting, then greet the user back.\n",
    "If the question is a goodbye message, then say \"Bye, See you later\" to the user back.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer. \n",
    "Do not repeat the answer. \n",
    "Do not mention the chapter number.\n",
    "Do not mention the S.no number.\n",
    "Do not mention the document number.\n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(\n",
    "    template)\n",
    "#LLM instance\n",
    "llm = ChatOpenAI(\n",
    "    temperature = 0, \n",
    "    model=llm_model)\n",
    "#Adding memory\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key='chat_history', \n",
    "    return_messages=True, \n",
    "    output_key='answer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_answer=[]\n",
    "chat_question=[]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/', methods=['GET', 'POST'])\n",
    "def index():\n",
    "    if request.method == 'POST':\n",
    "        \n",
    "        qa = ConversationalRetrievalChain.from_llm(\n",
    "            llm=llm, \n",
    "            retriever=vectorstore.as_retriever(),\n",
    "            memory=memory,\n",
    "            return_source_documents=True\n",
    "        )\n",
    "        \n",
    "        input_method = request.form.get('input_method')\n",
    "        if input_method == 'voice':\n",
    "            user_question = request.form['transcription']\n",
    "        else:\n",
    "            user_question = request.form['user_question']\n",
    "        formatted_prompt = template.format(context=\"\", question=user_question)\n",
    "        result = qa({\"question\": formatted_prompt})\n",
    "           \n",
    "        print(\"Question: \", user_question)\n",
    "        \n",
    "        chat_question.append(user_question)\n",
    "        chat_answer.append(result[\"answer\"])\n",
    "        print(\"Answer: \", result['answer'])\n",
    "        return render_template('index1.html', answer=result['answer'])\n",
    "    \n",
    "    return render_template('index1.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/conversation_history', methods=['GET'])\n",
    "def chat_history():\n",
    "    chat_length = len(chat_question)\n",
    "    return render_template('conversation_history.html', chat_question=chat_question, chat_answer=chat_answer, chat_length=chat_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
